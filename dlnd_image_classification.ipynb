{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAHF9JREFUeJzt3UmPZOl1HuAvxsyMrKzKqsqau6rYA5vNbropkjJJmYIs\nUIBXWtn+BV7YO/8Yr73wymtDNAwIggwSMEmBNMeW2Wz2VOzumquyco6M2QttzI2Bc5gChYPn2Z88\nEd+9cd+8q7ezWq0aAFBT9w/9AQCAfzyCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/T/0B/jH8l/+w79fZebGx9PwTK+f\n+3+pc/tGeGZvtJHa9faFYWruk1/+LDzznR/+PLVrbzILz/R6ybPvdFJzg7X18MylKzupXec34t/t\n83eupHb9+be+Hp6Zz+LXq7XWnu0fpeYGWxfDM+9+8NvUrr/97g/jQ8nnwNogN3dhMAjPDPuL1K5p\n4lrPZ7nfWFstU2NrvbXwzMkq/rxvrbUXp/F46eZ+Lu073/+75EH+P7t/3z8AAPzTJegBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+te3P84NddfxJuT\nBv1UUV67v5qEZ94f5yqQ3v7iK6m55TT+Ga/t5NraNlLfLXf22fa6k0n8PPZ3X6R2HXXiTWOT03Fq\n15e/+o3wzOzkNLXr2fPceVxbjzc3LqcHqV0ba/H7atlyrWtXt86l5r70ymvhmadP7qd2jceH4Zmj\no1xLYevGW/laa22tPw/P3Lx+IbVrNrwanvngV/dSu86CN3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpuPT9dScyfj/fDMsJMr92iLeKFCtzNMrXr2\n28epuZ88+Cw88+snudKS1SReSpEtp1lfX0/NzebxopnWzf0/vb4Rv4f3xrlilR+983545sblXCHI\nZJ67ZpkCo7XkE24wSHzG3NG3L7z6amruc3fuhme2t0apXY8e3gvPLGe55+K5izdSc4tBvPRotJYr\n3rm5Ey8i+rSXO/uz4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLLtdeNeriFrtxtvJ+ssJqldl/vx4z93/mJq1+lxvJWvtdb2DuPf7eB0ltq1\nSpz9YpFok2ut9ZKfsZ/533gWb11rrbXjafzsz61yu370i1+GZ15/7bXUrjdevZOa6w/j7V+f+1yu\nGe54OQjPPH74NLXr4HCcmmvrm+GRP/6zt1Orfv7j74VnxvN4G2VrrR3Oci1vz4/jz8ZL41zD3q3e\nYXjm9Cjb2vj780YPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAorW2qz1tlNzd0YxYsYtlu8AKO11i5d3AjPfLyKlym01trmxjI1t9aJl6SMOrnbara5Fp+Z\n58ppTie5IqJF4n/jjVGupGO4Fr+vrt++kdp186Xb4ZlnR7lCkEcHuRKXb3zj6+GZ3cePUrv+9b/5\nVnjmf/z3v07t+uEP/i41d+dLXw3PfPvtr6V2fXj/o/DMx9//cWrX/nQrNXc0jz/jvvjP42fYWmvj\n2YvwzM7OemrXWfBGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGGCHgAKE/QAUFjZ9rrhZu6rvbJ1NTzz8iq368Iw0Wa0/1lq12g73gzXWmvHw5PwzHKwSO364z+K\nN0lduxq/Xq219tEHH6TmPv3kfnim28u1G67m8Xa49W7u7P/kG/Gzfxq/NVprrf3oe99Nzb333p3w\nzGKc/JCbF8Mje8e5RsSjWe5964OHz8Mzx8teatfxPP4Zn+zlzmOyfi419/m7r4Rntq/dTO16+jx+\n9t/+9lupXWfBGz0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0BhZdvrjqa5xrALvc3wzOzZi9SuT/fiTWh/+uU3UrvG0+PU3K1lfGZ9tErt+uZ2/Ozf\nvLKT2nWyzH3GZ2vxFsCT/dz9sZjGZ/rTw9Suu598HJ7Z2Jundl26sp2am/39z8Iz2ebAH/7q3fDM\new8epHadznMtb/c/iTdZPnn+NLXr61/5Znjm7vbt1K7/9F//W2puOn4UnvnJj5+ldj1+/GF45qt/\nkXt2nwVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nsLKlNld666m5W60Xnjl/fiu16+cv4qUULyb7qV13r99Izf3bJy+HZwYHuQKdy+/Hz2Ptw4epXYvl\nLDX3uU58ZrBIDLXWuv34Pbzo5EpcJj/6aXjmQrKMZbkTLy9qrbXFPNGwdLBI7TrfOxeemRzn7vtL\n8UdOa6210Wocnjl49NvUrltffD08s7WZewZ//dVbqbkn+/EWqEdHJ6ldJye74ZmP3n8/tesseKMH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx7\n3Rtbo9Tc5vNn4ZleN9Gq1Vp7/aWXwjOHj5+mdrVVrkHtVmcVnhkNc7t6iUaozjL++VprLd5z9Q8m\n3cT/xsO11K7BKv7d+pmGt9baoBtv85tt5WrXVie51rv5JH4ei5a7F69143fItzdyrXzTzjA1t7h5\nLTyzfu9eatdJ5iMmWz3feuO11NyNk/g1uzGbp3a9/urN8MxrO/FGxLPijR4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21Gb3wUepuck8XoIx7uWKRE4u\nxEsONk7i5SOttXb67oepuUVvEZ6Zb+Zuq24vXkqxlixx6bT11Nw8UQ60WOY+42owiM+kNuXm+ldf\nSe3a2su9X5wmLtn07sXUrovzo/DM5mmuKmm+lytWOXqyH545efD91K6H//sX4Znzb72e2vX8Ua64\nazq6FJ6Zj1Or2snzF+GZg0G2Suv3540eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdc+P9lJznx6fhmfmy1z71LBzPTwzuriT2vV8fJiau95b\nC89snOb+f1wcxJv5JtNcm1/byZ3j5uuvhWdOE01orbV29OwgPLO2jLfrtdZabzIJz0ye5u6ptpZr\nlOtsx9se+51cn9/yIP4c2Hgr1+bXhvHv1Vproyfx6rXj+/dTu/Z+/UF4ZvnJ49SurUtbqbnd7XhL\n5PNHud/mwyefhWdeHt5I7ToL3ugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nMEEPAIUJegAoTNADQGGCHgAKK9te9+I03j7VWmuPTuJtRrOD49SunWtXwjOr21dTu9Yu5hqh1g7i\nzXz9B09Tu6ZHJ+GZoxZvrGqttcW5jdTc4O6d8Ey/s0jt2tyOn8fsN5+kds0SLYCn3Vxz4NafvZma\nO9l7Fh9679epXW2eeAd6mPh8rbXJMte0Obh+Mzxz/V9+M7VrbaMXntn9zYepXdsn8V2ttXbhbrxp\n85NHuYa9jV68FXEwGKZ2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLKlNrdvv5Sa6358PzyzMU6taotpvBhhrTNI7XpxfJCa+8Gnn4Vnbp4epna9\n0eIHOUmUsbTW2vh+/Dq31tr0p7+K72rx69xaa51bt8Izp69fT+06mY/CM2+/miunOe6eS82NH9wL\nzwz3c+VW8/PxApLpJ8lCoce5UqzB1SfhmZNruVKswaUL4ZmLf/HV1K69Tx+m5rZ34mU4Xz13N7Xr\nb/7Xi/DM2na8xOyseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAorGx73fWb11Jzh/efhWdGFzupXa2zFh4ZdHO7Hj57npr7z7/4P+GZL1zOtZP9\nx/XN8Mwo+a/q6vgoNbf7Try9bvdKvPmrtdY+msRbzabJprybr98Mz9y5mPte04ePU3PnEq1mneU0\ntasdxn9na92N1KqD8UlqbvHRR+GZ1YNHqV0vtuLPqs0v5BpEb778amru9FH8vroyij9zWmvtK196\nLTxz++XceZwFb/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoLCypTb7ixepuf5qPzwz6OeOcdqLF5DszcepXbvjXNnJfBX/bgeDXLnH/cEoPLO9mqd2Tbu5\nudVqEp7ZX+ZKSz57Ei+1Od9dT+16kbhkf3X/r1K7vnDrVmru1Uvx73Z57Xpq1/G9++GZxTh+vVpr\nbbXI3YsvXjxN7Mo9B6br8VKb2X68IKy11qa/fD81N0oUOk3WB6ldd998Kzwze/Db1K6z4I0eAAoT\n9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdcPV\nMjXXX87CMzvdXAPStBdvrerPpqldJ6e587h15Up45qWXb6d23T9KNPOtcm1cw2RrVWce/8lMl/HG\nu9Zau3F5JzzTzxWhtYOnj8Izq91cK9+D57mWt/3RMDxzZxL/PbfWWvdZvL2ujXOH353n3rfG8/g5\nnixyz49VohVxNO6kdj28/1lqbtSJ7zue567Z9iQ+t/P266ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQ\nA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGypzcZ4lJp7ML8QnrnaPU3tujjeC8/0\nnzxM7ZofvkjNffHNl8Mzd77w+dSu3V+8F5650emldrVBrgxnsIr/b7xxlCtx6bf4ZxyNNlK7fvPh\nvfDMznHuPeGVz11KzX02jBfUPP4g93vZONwNz3TmuXuqs8jdw6eJUqxpN3fNpsfxXbuLw9Su0eh8\nau5wGi+POp7krtnu/cfhmf6d66ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeA\nwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+v2j+NNV6219t39eEvT/HJqVfvWchqe2XjyKLVrfXaS\nmvvK174dnrl5+7XUru/86J3wzP4k1xy46Ofuj1miLW9j1UntOv0sfq17l3LNcK9c3AnPnC72U7v6\nm8PU3Nt/+vXwzG680Owf5n7yJDwzWeaa0Jb9tdTcOHFfbW4mH1Ybm+GR8TDXyre8fDE1d9ri+x49\njbcUttba/t6z8MyLX7+f2vWXqanf5Y0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtddODB6m5D54/Ds+MZ7k2ru2X4o1hXx7kWte2+vFWvtZa\ne/n27fDM+XO5BrXJIt7mNzmJz7TW2nCwSM2druL7ht3c/TGcxq/ZeDfXxtXtxx8Fy16ure3x81wD\n44t3fxWeGa3nGtQO18/FZzZGqV2Tc1upuePj4/DMaCf329ydxlsiD+e531h3Nk7NPXx0FN+1Hm/l\na621g1n8ObB5kGt7PAve6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhQl6AChM0ANAYWVLbf7V3VxZwdPdeJnFjz8+Se36m3vxkoONV3Lfa3RuLTW31YsXdcwO4wUYrbW2\n6MRLMI4nuV3rvdytv+gl/jfu5P6fXnbjc7vH8WKP1lpbncYLdIbHubOf7eWKiFYffhKeGSXfZaaj\n8+GZd+aT1K57z56k5taX8ZnhMlcYM1iP/146s05q1+lerpjpeBUvB+qfG6R2LQbx73b34nZq11nw\nRg8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY\n2fa612/mvtq/G90Jz9xeu5/a9T/fizeN/e29WWrXH929mZo7+vDj8Mxe8v/H3jJex7U3zTUHXhnF\nm65aa22x6oVnZsvcNXu6ip/Hs1G8fbG11k778fa6rU7uN7Z5IXf2y2n8M7bnB6lda2vxlsjPTnPN\ncM8Xq9Tc9UG8eW20mbs/tjbj57Ea59oNn01z59jvxZ8Fvd3c8+NLq2F45txh7jlwFrzRA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaTJJlJ5fWO+GZ\nP3l9J7Xr2XG8tOQn9/dTu959/CI19/lEUcd0mLutVsv4/52Hp5Pcrkm8lKK11gbr8e+2WuZKS1pi\nbmNtPbXqcBUvIDm4cy216/Jbb6TmevGfS3vnr7+X2nU7cV+9dPFKalebTFNj6/34gezPcoUxx8/j\nz9PryYKlmzuXU3PDbvy3OdjNPU/vHsYLyW5vb6d2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoA\nKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVba/r9HJfrTOPt1bd2M41hv2Lly+EZw6m\n8Zax1lq7t5dr8zvpxdv8rt6+ndrVG47CM6fzXDPc6eFhaq4/W4RnhoON1K743dHa/PHT1K7zi3l4\nZnKQu6d2Z4kautba9sWL8ZlO7l1mcBr/brc2N1O7hsn3rc7mWnxmkPuM3aN4w961fvz33FpriQLR\n1lpr3Un8t3mSfA5c6MXvj1fv5HLiLHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQ\nmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91qlatAWi0T7WTLeONda629eSl+/E9vnEvtOp7kPuN8\nHG/L27l8JbVr/Vy8r21vmWuvm01nqbl5Ym7SyzUOdju98Mz55L/umV6t6cF+btlp7jxWj56EZ15q\nuefAoBdv89sa587jai/Xbvgi0Ui5thVvAGytteUsfmPNT/ZSuw4muVbERHldW06OU7tuvHk1PPPy\nndxz8Sx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhZUttVl2cv/DLFq8SKTNcwUpF/rxwo2v3N5J7Xp+uJuamz5+GJ6ZHeeKIoab8XKP0+R1nq1yc91l\n/FovZom2jdZaZxG/P+bJ85gOMuUv8eKX1lrrzHPnsegN40PdXKnNYh7/bqtkWc/6YpCaW82m4ZlH\n67mimdla/OyXa6lVbbCZO4+Tk/h5DFfL1K4rd66HZ9b7ifv3jHijB4DCBD0AFCboAaAwQQ8AhQl6\nAChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91wYzM111sfhWeme0epXZlW\ns5vb8c/XWmv/bD/XrPXu3uPwzKMHn6R2HYwPwjNHy1z71Gk39z/uYLkKz8xXuba27ir+8zzu5Nra\nTlbxuX7yPWE5yV2z5SR+D3eS7XUtcZ1P+7nrvEw05bXW2nHmM65NUrtaN/7d1ge5+rrlIt5C11pr\nm8v4d3vt2lZq18Vh/OxPnueaA3Of8Hd5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhZUttWndXmqs0xmEZ/obqVXttDsLzwwSZQqttXbnRq4M5+PP4gUT\n08lxatdiGd+1N88VYDzr5G79rV78vuqscteskyio2c/1xbRH03hpSbeTe0/oJQp0srJvMoMWv86P\nl/Hfc2ut7bdcGc5R4lrfSpb8bCcKuHq7h6ld1/rrqbmv3b4ennn1du7hPRrHi8wmybIepTYAwP+X\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhdVtr1vm\n/oeZjE/CM9k2rk6iSWo1zTVkndvcTM3tnI83Lu0+fZLadfgoPrffy13nHySbxi4miujOJxoRW2tt\nM9FeN+vmmvIO5vG502TrWra7rteNX+thom2wtdZGqU+Z29Xv5CoHR4lrvZzNU7umi/h5bCTvjwvn\ncp+xzQ7CI0cvcmd/cD7+m+7Mc8+cndTU7/JGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrFMtfitUrMdZINasP+MDyzGucakFruONrVzfhn\n/Ok7f5/a9fzB0/DMvJO7hZ8mO9QO5vE2v9Ei2U6W+IhryXtxNYxf526iTa611jqJVr7WWuv3441h\ni1WynWwR/53N57m2tlXyMw4zx59sr1sm7qtuP/fQWbbcM27vaC8801vlzmOtuxWe6Sz/cHHrjR4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21KY7iBdg\ntNbaINHD0EkWxnR6ieNf5IozFsdHqbkbW6PwzOVB7jMOTsfhmfPLXEHKaSf3P243MTfv50pLjpfx\nuXHyXmyJEpfePLeskywU6iYKhVarZLlVJ372uW/V2qDTy80lnh8byfv+XGJss5N8DuTGWmvxwcn4\nOLUp8zgddePP0rPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJig\nB4DCBD0AFCboAaCwuu11/dxX660S//uscu1kLdVel2vl63dz3VrnOvHGsD9762Zq1/5JfNfPPnmW\n2vVsMk/NnS7jbWiTZK/ZMnF/LJP/uy8S36ubrG3sJGveut1sNV9cL9Hy1k9+vI1u7lk16safBVv9\n3OFvdePPuMvJdBklb5BBi/+mh8l7arWI7zpNtHOeFW/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwsqU2bbieHIyXFXRWyTaLRPHOfD5LrVomL3WmvOHG\nKLWq/eWXb4Vnrg1yhUIfPD5IzT0+jp//i3mupON02QvPTJK34rwTv86rRPFLa611e/Hv1VprvcRc\nsj+nDRIlP/1kt9VmptyqtbaWOP+1Tu5Dnu8twjMXkwU6m73cfbU+iJ9jP3crttks/hw46cTP8Kx4\noweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACis\ns8o2rwEA/+R5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAw\nQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM\n0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/xfkBwlHN40TWAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f0a79fcc0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array(x) / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    one_hot_encoded = np.zeros((len(x), 10))\n",
    "    for i, label in enumerate(x):\n",
    "        one_hot_encoded[i][label] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None] + list(image_shape), name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernel size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernel size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    x_tensor_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    filter_weights = tf.Variable(\n",
    "        tf.divide(\n",
    "            tf.truncated_normal([ *conv_ksize, x_tensor_shape[3], conv_num_outputs ]),\n",
    "            tf.sqrt(tf.constant(conv_num_outputs, dtype = tf.float32))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # convolution\n",
    "    x_tensor = tf.nn.conv2d(x_tensor,\n",
    "        filter = filter_weights,\n",
    "        strides = [1, *conv_strides, 1],\n",
    "        padding = 'SAME'\n",
    "    )\n",
    "    \n",
    "    # bias\n",
    "    x_tensor = tf.nn.bias_add(x_tensor, filter_bias)\n",
    "    \n",
    "    # nonlinearity\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    \n",
    "    # maxpool\n",
    "    x_tensor = tf.nn.max_pool(\n",
    "        x_tensor,\n",
    "        ksize = [1, *pool_ksize, 1],\n",
    "        strides = [1, *pool_strides, 1],\n",
    "        padding = 'SAME'\n",
    "    )\n",
    "    \n",
    "    return x_tensor\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    _, width, height, channels = x_tensor.get_shape().as_list()\n",
    "    x_tensor = tf.reshape(x_tensor, [-1, width * height * channels])\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs, activation = tf.nn.relu):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(\n",
    "        tf.divide(\n",
    "            tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs]),\n",
    "            tf.sqrt(tf.constant(num_outputs, dtype = tf.float32))\n",
    "        )\n",
    "    )\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    x_tensor = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    \n",
    "    if activation:\n",
    "        x_tensor = activation(x_tensor)\n",
    "        \n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return fully_conn(x_tensor, num_outputs, activation=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    x = conv2d_maxpool(x, 64, [5, 5], [1, 1], [2, 2], [2, 2])\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    x = conv2d_maxpool(x, 128, [3, 3], [1, 1], [2, 2], [2, 2])\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    x = conv2d_maxpool(x, 256, [3, 3], [1, 1], [2, 2], [2, 2])\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Apply a Flatten Layer\n",
    "    x = flatten(x)\n",
    "    \n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    x = fully_conn(x, 512)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    x = fully_conn(x, 256)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "    # Apply an Output Layer\n",
    "    x = output(x, 10)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict = { x: feature_batch, y: label_batch, keep_prob: keep_probability })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    out_accuracy = session.run(accuracy, { x: valid_features, y: valid_labels, keep_prob: 1.0 })\n",
    "    out_cost = session.run(cost, { x: feature_batch, y: label_batch, keep_prob: 1.0 })\n",
    "    print(\"Accuracy: \", out_accuracy, \"Cost: \", out_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy:  0.0928 Cost:  2.30343\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy:  0.0972 Cost:  2.30277\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy:  0.1012 Cost:  2.30273\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy:  0.1 Cost:  2.3026\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy:  0.1 Cost:  2.30264\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy:  0.0998 Cost:  2.30222\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy:  0.1046 Cost:  2.30029\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy:  0.1138 Cost:  2.29693\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy:  0.142 Cost:  2.27961\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy:  0.1606 Cost:  2.25228\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy:  0.163 Cost:  2.24222\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy:  0.1738 Cost:  2.22265\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy:  0.2104 Cost:  2.17869\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy:  0.2232 Cost:  2.14506\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy:  0.2584 Cost:  2.11254\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy:  0.269 Cost:  2.08689\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy:  0.2784 Cost:  2.05814\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy:  0.2746 Cost:  2.02719\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy:  0.293 Cost:  1.98196\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy:  0.3004 Cost:  1.97508\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy:  0.3186 Cost:  1.96901\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy:  0.3158 Cost:  1.9592\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy:  0.3362 Cost:  1.91817\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy:  0.3398 Cost:  1.92107\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy:  0.3364 Cost:  1.90671\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy:  0.3538 Cost:  1.87733\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy:  0.3558 Cost:  1.86007\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy:  0.3638 Cost:  1.83768\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy:  0.3496 Cost:  1.84607\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy:  0.3536 Cost:  1.80746\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy:  0.3664 Cost:  1.79852\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy:  0.3788 Cost:  1.75487\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy:  0.3726 Cost:  1.76558\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy:  0.3874 Cost:  1.76802\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy:  0.3686 Cost:  1.75802\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy:  0.3872 Cost:  1.70076\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy:  0.3954 Cost:  1.72046\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy:  0.3758 Cost:  1.72401\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy:  0.385 Cost:  1.70627\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy:  0.4022 Cost:  1.66053\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy:  0.397 Cost:  1.68885\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy:  0.4096 Cost:  1.62554\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy:  0.396 Cost:  1.66786\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy:  0.4136 Cost:  1.61234\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy:  0.42 Cost:  1.61094\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy:  0.4062 Cost:  1.62288\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy:  0.414 Cost:  1.58844\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy:  0.4232 Cost:  1.58056\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy:  0.4242 Cost:  1.5657\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy:  0.426 Cost:  1.57697\n",
      "Epoch 51, CIFAR-10 Batch 1:  Accuracy:  0.4258 Cost:  1.57702\n",
      "Epoch 52, CIFAR-10 Batch 1:  Accuracy:  0.4222 Cost:  1.55541\n",
      "Epoch 53, CIFAR-10 Batch 1:  Accuracy:  0.43 Cost:  1.56604\n",
      "Epoch 54, CIFAR-10 Batch 1:  Accuracy:  0.4314 Cost:  1.55244\n",
      "Epoch 55, CIFAR-10 Batch 1:  Accuracy:  0.4392 Cost:  1.5104\n",
      "Epoch 56, CIFAR-10 Batch 1:  Accuracy:  0.4274 Cost:  1.56173\n",
      "Epoch 57, CIFAR-10 Batch 1:  Accuracy:  0.4268 Cost:  1.54979\n",
      "Epoch 58, CIFAR-10 Batch 1:  Accuracy:  0.4388 Cost:  1.52518\n",
      "Epoch 59, CIFAR-10 Batch 1:  Accuracy:  0.4416 Cost:  1.49912\n",
      "Epoch 60, CIFAR-10 Batch 1:  Accuracy:  0.4404 Cost:  1.47309\n",
      "Epoch 61, CIFAR-10 Batch 1:  Accuracy:  0.4434 Cost:  1.51502\n",
      "Epoch 62, CIFAR-10 Batch 1:  Accuracy:  0.4386 Cost:  1.50204\n",
      "Epoch 63, CIFAR-10 Batch 1:  Accuracy:  0.44 Cost:  1.51081\n",
      "Epoch 64, CIFAR-10 Batch 1:  Accuracy:  0.4402 Cost:  1.49476\n",
      "Epoch 65, CIFAR-10 Batch 1:  Accuracy:  0.4548 Cost:  1.47059\n",
      "Epoch 66, CIFAR-10 Batch 1:  Accuracy:  0.4548 Cost:  1.47902\n",
      "Epoch 67, CIFAR-10 Batch 1:  Accuracy:  0.4466 Cost:  1.4426\n",
      "Epoch 68, CIFAR-10 Batch 1:  Accuracy:  0.468 Cost:  1.40573\n",
      "Epoch 69, CIFAR-10 Batch 1:  Accuracy:  0.4518 Cost:  1.43005\n",
      "Epoch 70, CIFAR-10 Batch 1:  Accuracy:  0.4614 Cost:  1.39053\n",
      "Epoch 71, CIFAR-10 Batch 1:  Accuracy:  0.4582 Cost:  1.41434\n",
      "Epoch 72, CIFAR-10 Batch 1:  Accuracy:  0.445 Cost:  1.43639\n",
      "Epoch 73, CIFAR-10 Batch 1:  Accuracy:  0.458 Cost:  1.47217\n",
      "Epoch 74, CIFAR-10 Batch 1:  Accuracy:  0.466 Cost:  1.41901\n",
      "Epoch 75, CIFAR-10 Batch 1:  Accuracy:  0.461 Cost:  1.41932\n",
      "Epoch 76, CIFAR-10 Batch 1:  Accuracy:  0.4628 Cost:  1.36828\n",
      "Epoch 77, CIFAR-10 Batch 1:  Accuracy:  0.4708 Cost:  1.34934\n",
      "Epoch 78, CIFAR-10 Batch 1:  Accuracy:  0.4658 Cost:  1.34971\n",
      "Epoch 79, CIFAR-10 Batch 1:  Accuracy:  0.4796 Cost:  1.33184\n",
      "Epoch 80, CIFAR-10 Batch 1:  Accuracy:  0.4668 Cost:  1.36012\n",
      "Epoch 81, CIFAR-10 Batch 1:  Accuracy:  0.4796 Cost:  1.35173\n",
      "Epoch 82, CIFAR-10 Batch 1:  Accuracy:  0.482 Cost:  1.335\n",
      "Epoch 83, CIFAR-10 Batch 1:  Accuracy:  0.4646 Cost:  1.36838\n",
      "Epoch 84, CIFAR-10 Batch 1:  Accuracy:  0.4826 Cost:  1.35176\n",
      "Epoch 85, CIFAR-10 Batch 1:  Accuracy:  0.4726 Cost:  1.39792\n",
      "Epoch 86, CIFAR-10 Batch 1:  Accuracy:  0.4716 Cost:  1.37002\n",
      "Epoch 87, CIFAR-10 Batch 1:  Accuracy:  0.4856 Cost:  1.31699\n",
      "Epoch 88, CIFAR-10 Batch 1:  Accuracy:  0.487 Cost:  1.327\n",
      "Epoch 89, CIFAR-10 Batch 1:  Accuracy:  0.488 Cost:  1.27563\n",
      "Epoch 90, CIFAR-10 Batch 1:  Accuracy:  0.4946 Cost:  1.27916\n",
      "Epoch 91, CIFAR-10 Batch 1:  Accuracy:  0.493 Cost:  1.29273\n",
      "Epoch 92, CIFAR-10 Batch 1:  Accuracy:  0.4986 Cost:  1.25727\n",
      "Epoch 93, CIFAR-10 Batch 1:  Accuracy:  0.4958 Cost:  1.27746\n",
      "Epoch 94, CIFAR-10 Batch 1:  Accuracy:  0.5052 Cost:  1.23236\n",
      "Epoch 95, CIFAR-10 Batch 1:  Accuracy:  0.512 Cost:  1.21345\n",
      "Epoch 96, CIFAR-10 Batch 1:  Accuracy:  0.508 Cost:  1.24639\n",
      "Epoch 97, CIFAR-10 Batch 1:  Accuracy:  0.4978 Cost:  1.2716\n",
      "Epoch 98, CIFAR-10 Batch 1:  Accuracy:  0.5248 Cost:  1.17533\n",
      "Epoch 99, CIFAR-10 Batch 1:  Accuracy:  0.5302 Cost:  1.21467\n",
      "Epoch 100, CIFAR-10 Batch 1:  Accuracy:  0.5152 Cost:  1.18628\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Accuracy:  0.102 Cost:  2.30279\n",
      "Epoch  1, CIFAR-10 Batch 2:  Accuracy:  0.101 Cost:  2.30291\n",
      "Epoch  1, CIFAR-10 Batch 3:  Accuracy:  0.0998 Cost:  2.30282\n",
      "Epoch  1, CIFAR-10 Batch 4:  Accuracy:  0.1 Cost:  2.30209\n",
      "Epoch  1, CIFAR-10 Batch 5:  Accuracy:  0.1 Cost:  2.30189\n",
      "Epoch  2, CIFAR-10 Batch 1:  Accuracy:  0.0998 Cost:  2.30344\n",
      "Epoch  2, CIFAR-10 Batch 2:  Accuracy:  0.1 Cost:  2.30313\n",
      "Epoch  2, CIFAR-10 Batch 3:  Accuracy:  0.0998 Cost:  2.30277\n",
      "Epoch  2, CIFAR-10 Batch 4:  Accuracy:  0.1008 Cost:  2.30156\n",
      "Epoch  2, CIFAR-10 Batch 5:  Accuracy:  0.0972 Cost:  2.30023\n",
      "Epoch  3, CIFAR-10 Batch 1:  Accuracy:  0.122 Cost:  2.3009\n",
      "Epoch  3, CIFAR-10 Batch 2:  Accuracy:  0.1588 Cost:  2.28802\n",
      "Epoch  3, CIFAR-10 Batch 3:  Accuracy:  0.1694 Cost:  2.25248\n",
      "Epoch  3, CIFAR-10 Batch 4:  Accuracy:  0.1608 Cost:  2.23421\n",
      "Epoch  3, CIFAR-10 Batch 5:  Accuracy:  0.1648 Cost:  2.20516\n",
      "Epoch  4, CIFAR-10 Batch 1:  Accuracy:  0.1638 Cost:  2.20998\n",
      "Epoch  4, CIFAR-10 Batch 2:  Accuracy:  0.1852 Cost:  2.21031\n",
      "Epoch  4, CIFAR-10 Batch 3:  Accuracy:  0.2066 Cost:  2.17519\n",
      "Epoch  4, CIFAR-10 Batch 4:  Accuracy:  0.1912 Cost:  2.14923\n",
      "Epoch  4, CIFAR-10 Batch 5:  Accuracy:  0.2156 Cost:  2.12093\n",
      "Epoch  5, CIFAR-10 Batch 1:  Accuracy:  0.2318 Cost:  2.14001\n",
      "Epoch  5, CIFAR-10 Batch 2:  Accuracy:  0.2028 Cost:  2.15745\n",
      "Epoch  5, CIFAR-10 Batch 3:  Accuracy:  0.2426 Cost:  2.0641\n",
      "Epoch  5, CIFAR-10 Batch 4:  Accuracy:  0.247 Cost:  2.03007\n",
      "Epoch  5, CIFAR-10 Batch 5:  Accuracy:  0.2414 Cost:  1.99354\n",
      "Epoch  6, CIFAR-10 Batch 1:  Accuracy:  0.3116 Cost:  2.01365\n",
      "Epoch  6, CIFAR-10 Batch 2:  Accuracy:  0.2674 Cost:  2.03118\n",
      "Epoch  6, CIFAR-10 Batch 3:  Accuracy:  0.292 Cost:  1.90779\n",
      "Epoch  6, CIFAR-10 Batch 4:  Accuracy:  0.3038 Cost:  1.8591\n",
      "Epoch  6, CIFAR-10 Batch 5:  Accuracy:  0.3074 Cost:  1.89384\n",
      "Epoch  7, CIFAR-10 Batch 1:  Accuracy:  0.3356 Cost:  1.94569\n",
      "Epoch  7, CIFAR-10 Batch 2:  Accuracy:  0.285 Cost:  1.96708\n",
      "Epoch  7, CIFAR-10 Batch 3:  Accuracy:  0.3262 Cost:  1.75062\n",
      "Epoch  7, CIFAR-10 Batch 4:  Accuracy:  0.3372 Cost:  1.82644\n",
      "Epoch  7, CIFAR-10 Batch 5:  Accuracy:  0.326 Cost:  1.85834\n",
      "Epoch  8, CIFAR-10 Batch 1:  Accuracy:  0.3676 Cost:  1.88143\n",
      "Epoch  8, CIFAR-10 Batch 2:  Accuracy:  0.347 Cost:  1.82281\n",
      "Epoch  8, CIFAR-10 Batch 3:  Accuracy:  0.3588 Cost:  1.69275\n",
      "Epoch  8, CIFAR-10 Batch 4:  Accuracy:  0.3656 Cost:  1.75761\n",
      "Epoch  8, CIFAR-10 Batch 5:  Accuracy:  0.3604 Cost:  1.7476\n",
      "Epoch  9, CIFAR-10 Batch 1:  Accuracy:  0.38 Cost:  1.7819\n",
      "Epoch  9, CIFAR-10 Batch 2:  Accuracy:  0.3524 Cost:  1.79147\n",
      "Epoch  9, CIFAR-10 Batch 3:  Accuracy:  0.3634 Cost:  1.63692\n",
      "Epoch  9, CIFAR-10 Batch 4:  Accuracy:  0.3686 Cost:  1.69859\n",
      "Epoch  9, CIFAR-10 Batch 5:  Accuracy:  0.3974 Cost:  1.67575\n",
      "Epoch 10, CIFAR-10 Batch 1:  Accuracy:  0.403 Cost:  1.7324\n",
      "Epoch 10, CIFAR-10 Batch 2:  Accuracy:  0.359 Cost:  1.79823\n",
      "Epoch 10, CIFAR-10 Batch 3:  Accuracy:  0.3926 Cost:  1.57466\n",
      "Epoch 10, CIFAR-10 Batch 4:  Accuracy:  0.393 Cost:  1.62309\n",
      "Epoch 10, CIFAR-10 Batch 5:  Accuracy:  0.3994 Cost:  1.64283\n",
      "Epoch 11, CIFAR-10 Batch 1:  Accuracy:  0.4172 Cost:  1.72628\n",
      "Epoch 11, CIFAR-10 Batch 2:  Accuracy:  0.3798 Cost:  1.70344\n",
      "Epoch 11, CIFAR-10 Batch 3:  Accuracy:  0.3874 Cost:  1.55252\n",
      "Epoch 11, CIFAR-10 Batch 4:  Accuracy:  0.407 Cost:  1.5893\n",
      "Epoch 11, CIFAR-10 Batch 5:  Accuracy:  0.412 Cost:  1.58714\n",
      "Epoch 12, CIFAR-10 Batch 1:  Accuracy:  0.4344 Cost:  1.68183\n",
      "Epoch 12, CIFAR-10 Batch 2:  Accuracy:  0.4114 Cost:  1.60093\n",
      "Epoch 12, CIFAR-10 Batch 3:  Accuracy:  0.4026 Cost:  1.50224\n",
      "Epoch 12, CIFAR-10 Batch 4:  Accuracy:  0.4088 Cost:  1.57016\n",
      "Epoch 12, CIFAR-10 Batch 5:  Accuracy:  0.4386 Cost:  1.51194\n",
      "Epoch 13, CIFAR-10 Batch 1:  Accuracy:  0.4326 Cost:  1.62552\n",
      "Epoch 13, CIFAR-10 Batch 2:  Accuracy:  0.4234 Cost:  1.60517\n",
      "Epoch 13, CIFAR-10 Batch 3:  Accuracy:  0.4176 Cost:  1.4775\n",
      "Epoch 13, CIFAR-10 Batch 4:  Accuracy:  0.434 Cost:  1.49836\n",
      "Epoch 13, CIFAR-10 Batch 5:  Accuracy:  0.449 Cost:  1.49211\n",
      "Epoch 14, CIFAR-10 Batch 1:  Accuracy:  0.4454 Cost:  1.62066\n",
      "Epoch 14, CIFAR-10 Batch 2:  Accuracy:  0.4034 Cost:  1.63231\n",
      "Epoch 14, CIFAR-10 Batch 3:  Accuracy:  0.421 Cost:  1.4734\n",
      "Epoch 14, CIFAR-10 Batch 4:  Accuracy:  0.4242 Cost:  1.51787\n",
      "Epoch 14, CIFAR-10 Batch 5:  Accuracy:  0.4396 Cost:  1.5058\n",
      "Epoch 15, CIFAR-10 Batch 1:  Accuracy:  0.462 Cost:  1.57624\n",
      "Epoch 15, CIFAR-10 Batch 2:  Accuracy:  0.4262 Cost:  1.59606\n",
      "Epoch 15, CIFAR-10 Batch 3:  Accuracy:  0.4428 Cost:  1.39197\n",
      "Epoch 15, CIFAR-10 Batch 4:  Accuracy:  0.431 Cost:  1.48086\n",
      "Epoch 15, CIFAR-10 Batch 5:  Accuracy:  0.4558 Cost:  1.47428\n",
      "Epoch 16, CIFAR-10 Batch 1:  Accuracy:  0.4674 Cost:  1.55597\n",
      "Epoch 16, CIFAR-10 Batch 2:  Accuracy:  0.4428 Cost:  1.53285\n",
      "Epoch 16, CIFAR-10 Batch 3:  Accuracy:  0.437 Cost:  1.38786\n",
      "Epoch 16, CIFAR-10 Batch 4:  Accuracy:  0.451 Cost:  1.47191\n",
      "Epoch 16, CIFAR-10 Batch 5:  Accuracy:  0.4784 Cost:  1.42351\n",
      "Epoch 17, CIFAR-10 Batch 1:  Accuracy:  0.4808 Cost:  1.52231\n",
      "Epoch 17, CIFAR-10 Batch 2:  Accuracy:  0.4624 Cost:  1.46618\n",
      "Epoch 17, CIFAR-10 Batch 3:  Accuracy:  0.4756 Cost:  1.31033\n",
      "Epoch 17, CIFAR-10 Batch 4:  Accuracy:  0.466 Cost:  1.40043\n",
      "Epoch 17, CIFAR-10 Batch 5:  Accuracy:  0.4684 Cost:  1.41658\n",
      "Epoch 18, CIFAR-10 Batch 1:  Accuracy:  0.492 Cost:  1.49169\n",
      "Epoch 18, CIFAR-10 Batch 2:  Accuracy:  0.4692 Cost:  1.46267\n",
      "Epoch 18, CIFAR-10 Batch 3:  Accuracy:  0.4796 Cost:  1.28988\n",
      "Epoch 18, CIFAR-10 Batch 4:  Accuracy:  0.4854 Cost:  1.33604\n",
      "Epoch 18, CIFAR-10 Batch 5:  Accuracy:  0.467 Cost:  1.41668\n",
      "Epoch 19, CIFAR-10 Batch 1:  Accuracy:  0.4916 Cost:  1.47648\n",
      "Epoch 19, CIFAR-10 Batch 2:  Accuracy:  0.4876 Cost:  1.43899\n",
      "Epoch 19, CIFAR-10 Batch 3:  Accuracy:  0.4776 Cost:  1.26949\n",
      "Epoch 19, CIFAR-10 Batch 4:  Accuracy:  0.4954 Cost:  1.33003\n",
      "Epoch 19, CIFAR-10 Batch 5:  Accuracy:  0.5018 Cost:  1.33749\n",
      "Epoch 20, CIFAR-10 Batch 1:  Accuracy:  0.511 Cost:  1.45108\n",
      "Epoch 20, CIFAR-10 Batch 2:  Accuracy:  0.497 Cost:  1.38877\n",
      "Epoch 20, CIFAR-10 Batch 3:  Accuracy:  0.5038 Cost:  1.21272\n",
      "Epoch 20, CIFAR-10 Batch 4:  Accuracy:  0.487 Cost:  1.3405\n",
      "Epoch 20, CIFAR-10 Batch 5:  Accuracy:  0.4996 Cost:  1.34287\n",
      "Epoch 21, CIFAR-10 Batch 1:  Accuracy:  0.5276 Cost:  1.39569\n",
      "Epoch 21, CIFAR-10 Batch 2:  Accuracy:  0.498 Cost:  1.43639\n",
      "Epoch 21, CIFAR-10 Batch 3:  Accuracy:  0.5042 Cost:  1.23909\n",
      "Epoch 21, CIFAR-10 Batch 4:  Accuracy:  0.5166 Cost:  1.25642\n",
      "Epoch 21, CIFAR-10 Batch 5:  Accuracy:  0.516 Cost:  1.30178\n",
      "Epoch 22, CIFAR-10 Batch 1:  Accuracy:  0.5192 Cost:  1.42469\n",
      "Epoch 22, CIFAR-10 Batch 2:  Accuracy:  0.5002 Cost:  1.41876\n",
      "Epoch 22, CIFAR-10 Batch 3:  Accuracy:  0.5188 Cost:  1.16077\n",
      "Epoch 22, CIFAR-10 Batch 4:  Accuracy:  0.537 Cost:  1.21123\n",
      "Epoch 22, CIFAR-10 Batch 5:  Accuracy:  0.5148 Cost:  1.28805\n",
      "Epoch 23, CIFAR-10 Batch 1:  Accuracy:  0.5326 Cost:  1.37083\n",
      "Epoch 23, CIFAR-10 Batch 2:  Accuracy:  0.5164 Cost:  1.35696\n",
      "Epoch 23, CIFAR-10 Batch 3:  Accuracy:  0.5198 Cost:  1.17899\n",
      "Epoch 23, CIFAR-10 Batch 4:  Accuracy:  0.5328 Cost:  1.19111\n",
      "Epoch 23, CIFAR-10 Batch 5:  Accuracy:  0.5208 Cost:  1.27332\n",
      "Epoch 24, CIFAR-10 Batch 1:  Accuracy:  0.5204 Cost:  1.37408\n",
      "Epoch 24, CIFAR-10 Batch 2:  Accuracy:  0.499 Cost:  1.4219\n",
      "Epoch 24, CIFAR-10 Batch 3:  Accuracy:  0.5058 Cost:  1.1737\n",
      "Epoch 24, CIFAR-10 Batch 4:  Accuracy:  0.525 Cost:  1.21373\n",
      "Epoch 24, CIFAR-10 Batch 5:  Accuracy:  0.5344 Cost:  1.24433\n",
      "Epoch 25, CIFAR-10 Batch 1:  Accuracy:  0.555 Cost:  1.28821\n",
      "Epoch 25, CIFAR-10 Batch 2:  Accuracy:  0.5398 Cost:  1.2763\n",
      "Epoch 25, CIFAR-10 Batch 3:  Accuracy:  0.526 Cost:  1.15636\n",
      "Epoch 25, CIFAR-10 Batch 4:  Accuracy:  0.5522 Cost:  1.15662\n",
      "Epoch 25, CIFAR-10 Batch 5:  Accuracy:  0.5422 Cost:  1.22361\n",
      "Epoch 26, CIFAR-10 Batch 1:  Accuracy:  0.56 Cost:  1.24228\n",
      "Epoch 26, CIFAR-10 Batch 2:  Accuracy:  0.5552 Cost:  1.20854\n",
      "Epoch 26, CIFAR-10 Batch 3:  Accuracy:  0.5404 Cost:  1.12133\n",
      "Epoch 26, CIFAR-10 Batch 4:  Accuracy:  0.5512 Cost:  1.12874\n",
      "Epoch 26, CIFAR-10 Batch 5:  Accuracy:  0.5702 Cost:  1.16023\n",
      "Epoch 27, CIFAR-10 Batch 1:  Accuracy:  0.5614 Cost:  1.23034\n",
      "Epoch 27, CIFAR-10 Batch 2:  Accuracy:  0.5566 Cost:  1.23859\n",
      "Epoch 27, CIFAR-10 Batch 3:  Accuracy:  0.5272 Cost:  1.14289\n",
      "Epoch 27, CIFAR-10 Batch 4:  Accuracy:  0.5566 Cost:  1.11596\n",
      "Epoch 27, CIFAR-10 Batch 5:  Accuracy:  0.5748 Cost:  1.12244\n",
      "Epoch 28, CIFAR-10 Batch 1:  Accuracy:  0.5758 Cost:  1.22805\n",
      "Epoch 28, CIFAR-10 Batch 2:  Accuracy:  0.5644 Cost:  1.20968\n",
      "Epoch 28, CIFAR-10 Batch 3:  Accuracy:  0.55 Cost:  1.06898\n",
      "Epoch 28, CIFAR-10 Batch 4:  Accuracy:  0.5696 Cost:  1.07341\n",
      "Epoch 28, CIFAR-10 Batch 5:  Accuracy:  0.5748 Cost:  1.12316\n",
      "Epoch 29, CIFAR-10 Batch 1:  Accuracy:  0.5768 Cost:  1.18641\n",
      "Epoch 29, CIFAR-10 Batch 2:  Accuracy:  0.5824 Cost:  1.14874\n",
      "Epoch 29, CIFAR-10 Batch 3:  Accuracy:  0.5678 Cost:  1.05754\n",
      "Epoch 29, CIFAR-10 Batch 4:  Accuracy:  0.578 Cost:  1.0386\n",
      "Epoch 29, CIFAR-10 Batch 5:  Accuracy:  0.5734 Cost:  1.11428\n",
      "Epoch 30, CIFAR-10 Batch 1:  Accuracy:  0.5914 Cost:  1.13722\n",
      "Epoch 30, CIFAR-10 Batch 2:  Accuracy:  0.5832 Cost:  1.17139\n",
      "Epoch 30, CIFAR-10 Batch 3:  Accuracy:  0.5814 Cost:  1.00535\n",
      "Epoch 30, CIFAR-10 Batch 4:  Accuracy:  0.5732 Cost:  1.06277\n",
      "Epoch 30, CIFAR-10 Batch 5:  Accuracy:  0.5718 Cost:  1.11795\n",
      "Epoch 31, CIFAR-10 Batch 1:  Accuracy:  0.5996 Cost:  1.11753\n",
      "Epoch 31, CIFAR-10 Batch 2:  Accuracy:  0.5856 Cost:  1.14407\n",
      "Epoch 31, CIFAR-10 Batch 3:  Accuracy:  0.5596 Cost:  1.04431\n",
      "Epoch 31, CIFAR-10 Batch 4:  Accuracy:  0.6008 Cost:  0.998101\n",
      "Epoch 31, CIFAR-10 Batch 5:  Accuracy:  0.5796 Cost:  1.07525\n",
      "Epoch 32, CIFAR-10 Batch 1:  Accuracy:  0.5934 Cost:  1.14904\n",
      "Epoch 32, CIFAR-10 Batch 2:  Accuracy:  0.5872 Cost:  1.16401\n",
      "Epoch 32, CIFAR-10 Batch 3:  Accuracy:  0.5908 Cost:  0.964786\n",
      "Epoch 32, CIFAR-10 Batch 4:  Accuracy:  0.5718 Cost:  1.05703\n",
      "Epoch 32, CIFAR-10 Batch 5:  Accuracy:  0.5954 Cost:  1.04231\n",
      "Epoch 33, CIFAR-10 Batch 1:  Accuracy:  0.603 Cost:  1.10062\n",
      "Epoch 33, CIFAR-10 Batch 2:  Accuracy:  0.6046 Cost:  1.0817\n",
      "Epoch 33, CIFAR-10 Batch 3:  Accuracy:  0.5784 Cost:  1.01055\n",
      "Epoch 33, CIFAR-10 Batch 4:  Accuracy:  0.5928 Cost:  1.01902\n",
      "Epoch 33, CIFAR-10 Batch 5:  Accuracy:  0.5988 Cost:  1.03126\n",
      "Epoch 34, CIFAR-10 Batch 1:  Accuracy:  0.61 Cost:  1.10847\n",
      "Epoch 34, CIFAR-10 Batch 2:  Accuracy:  0.6056 Cost:  1.08293\n",
      "Epoch 34, CIFAR-10 Batch 3:  Accuracy:  0.577 Cost:  1.00119\n",
      "Epoch 34, CIFAR-10 Batch 4:  Accuracy:  0.6048 Cost:  0.991029\n",
      "Epoch 34, CIFAR-10 Batch 5:  Accuracy:  0.5952 Cost:  1.03938\n",
      "Epoch 35, CIFAR-10 Batch 1:  Accuracy:  0.6084 Cost:  1.07452\n",
      "Epoch 35, CIFAR-10 Batch 2:  Accuracy:  0.6122 Cost:  1.06059\n",
      "Epoch 35, CIFAR-10 Batch 3:  Accuracy:  0.5992 Cost:  0.949945\n",
      "Epoch 35, CIFAR-10 Batch 4:  Accuracy:  0.6128 Cost:  0.957953\n",
      "Epoch 35, CIFAR-10 Batch 5:  Accuracy:  0.611 Cost:  1.0105\n",
      "Epoch 36, CIFAR-10 Batch 1:  Accuracy:  0.6192 Cost:  1.0493\n",
      "Epoch 36, CIFAR-10 Batch 2:  Accuracy:  0.6338 Cost:  0.996101\n",
      "Epoch 36, CIFAR-10 Batch 3:  Accuracy:  0.5936 Cost:  0.975051\n",
      "Epoch 36, CIFAR-10 Batch 4:  Accuracy:  0.6194 Cost:  0.929798\n",
      "Epoch 36, CIFAR-10 Batch 5:  Accuracy:  0.6128 Cost:  0.993563\n",
      "Epoch 37, CIFAR-10 Batch 1:  Accuracy:  0.6326 Cost:  1.05773\n",
      "Epoch 37, CIFAR-10 Batch 2:  Accuracy:  0.639 Cost:  0.999962\n",
      "Epoch 37, CIFAR-10 Batch 3:  Accuracy:  0.6074 Cost:  0.907648\n",
      "Epoch 37, CIFAR-10 Batch 4:  Accuracy:  0.6272 Cost:  0.917004\n",
      "Epoch 37, CIFAR-10 Batch 5:  Accuracy:  0.6062 Cost:  0.996351\n",
      "Epoch 38, CIFAR-10 Batch 1:  Accuracy:  0.643 Cost:  1.00885\n",
      "Epoch 38, CIFAR-10 Batch 2:  Accuracy:  0.6426 Cost:  0.952866\n",
      "Epoch 38, CIFAR-10 Batch 3:  Accuracy:  0.6182 Cost:  0.901057\n",
      "Epoch 38, CIFAR-10 Batch 4:  Accuracy:  0.6142 Cost:  0.931084\n",
      "Epoch 38, CIFAR-10 Batch 5:  Accuracy:  0.6234 Cost:  0.96981\n",
      "Epoch 39, CIFAR-10 Batch 1:  Accuracy:  0.6336 Cost:  1.00002\n",
      "Epoch 39, CIFAR-10 Batch 2:  Accuracy:  0.6372 Cost:  0.981423\n",
      "Epoch 39, CIFAR-10 Batch 3:  Accuracy:  0.6244 Cost:  0.892657\n",
      "Epoch 39, CIFAR-10 Batch 4:  Accuracy:  0.6284 Cost:  0.907738\n",
      "Epoch 39, CIFAR-10 Batch 5:  Accuracy:  0.6248 Cost:  0.94549\n",
      "Epoch 40, CIFAR-10 Batch 1:  Accuracy:  0.6404 Cost:  0.964613\n",
      "Epoch 40, CIFAR-10 Batch 2:  Accuracy:  0.6458 Cost:  0.949865\n",
      "Epoch 40, CIFAR-10 Batch 3:  Accuracy:  0.6056 Cost:  0.91384\n",
      "Epoch 40, CIFAR-10 Batch 4:  Accuracy:  0.6372 Cost:  0.874353\n",
      "Epoch 40, CIFAR-10 Batch 5:  Accuracy:  0.6378 Cost:  0.914378\n",
      "Epoch 41, CIFAR-10 Batch 1:  Accuracy:  0.6436 Cost:  0.957222\n",
      "Epoch 41, CIFAR-10 Batch 2:  Accuracy:  0.645 Cost:  0.944482\n",
      "Epoch 41, CIFAR-10 Batch 3:  Accuracy:  0.633 Cost:  0.855816\n",
      "Epoch 41, CIFAR-10 Batch 4:  Accuracy:  0.628 Cost:  0.882375\n",
      "Epoch 41, CIFAR-10 Batch 5:  Accuracy:  0.653 Cost:  0.876949\n",
      "Epoch 42, CIFAR-10 Batch 1:  Accuracy:  0.6598 Cost:  0.914388\n",
      "Epoch 42, CIFAR-10 Batch 2:  Accuracy:  0.6516 Cost:  0.959088\n",
      "Epoch 42, CIFAR-10 Batch 3:  Accuracy:  0.632 Cost:  0.867812\n",
      "Epoch 42, CIFAR-10 Batch 4:  Accuracy:  0.6344 Cost:  0.878637\n",
      "Epoch 42, CIFAR-10 Batch 5:  Accuracy:  0.6368 Cost:  0.903036\n",
      "Epoch 43, CIFAR-10 Batch 1:  Accuracy:  0.6414 Cost:  0.953067\n",
      "Epoch 43, CIFAR-10 Batch 2:  Accuracy:  0.6552 Cost:  0.905622\n",
      "Epoch 43, CIFAR-10 Batch 3:  Accuracy:  0.6338 Cost:  0.848497\n",
      "Epoch 43, CIFAR-10 Batch 4:  Accuracy:  0.6206 Cost:  0.885241\n",
      "Epoch 43, CIFAR-10 Batch 5:  Accuracy:  0.6384 Cost:  0.890857\n",
      "Epoch 44, CIFAR-10 Batch 1:  Accuracy:  0.6622 Cost:  0.901148\n",
      "Epoch 44, CIFAR-10 Batch 2:  Accuracy:  0.6486 Cost:  0.917094\n",
      "Epoch 44, CIFAR-10 Batch 3:  Accuracy:  0.6368 Cost:  0.829781\n",
      "Epoch 44, CIFAR-10 Batch 4:  Accuracy:  0.646 Cost:  0.849592\n",
      "Epoch 44, CIFAR-10 Batch 5:  Accuracy:  0.6302 Cost:  0.902046\n",
      "Epoch 45, CIFAR-10 Batch 1:  Accuracy:  0.6636 Cost:  0.912285\n",
      "Epoch 45, CIFAR-10 Batch 2:  Accuracy:  0.6622 Cost:  0.880741\n",
      "Epoch 45, CIFAR-10 Batch 3:  Accuracy:  0.6298 Cost:  0.838078\n",
      "Epoch 45, CIFAR-10 Batch 4:  Accuracy:  0.6476 Cost:  0.846873\n",
      "Epoch 45, CIFAR-10 Batch 5:  Accuracy:  0.6536 Cost:  0.852882\n",
      "Epoch 46, CIFAR-10 Batch 1:  Accuracy:  0.6654 Cost:  0.914687\n",
      "Epoch 46, CIFAR-10 Batch 2:  Accuracy:  0.6672 Cost:  0.863698\n",
      "Epoch 46, CIFAR-10 Batch 3:  Accuracy:  0.6468 Cost:  0.794073\n",
      "Epoch 46, CIFAR-10 Batch 4:  Accuracy:  0.657 Cost:  0.807971\n",
      "Epoch 46, CIFAR-10 Batch 5:  Accuracy:  0.659 Cost:  0.853757\n",
      "Epoch 47, CIFAR-10 Batch 1:  Accuracy:  0.6746 Cost:  0.887911\n",
      "Epoch 47, CIFAR-10 Batch 2:  Accuracy:  0.6642 Cost:  0.86829\n",
      "Epoch 47, CIFAR-10 Batch 3:  Accuracy:  0.6506 Cost:  0.79792\n",
      "Epoch 47, CIFAR-10 Batch 4:  Accuracy:  0.6586 Cost:  0.798022\n",
      "Epoch 47, CIFAR-10 Batch 5:  Accuracy:  0.6638 Cost:  0.830519\n",
      "Epoch 48, CIFAR-10 Batch 1:  Accuracy:  0.6646 Cost:  0.882647\n",
      "Epoch 48, CIFAR-10 Batch 2:  Accuracy:  0.6624 Cost:  0.846468\n",
      "Epoch 48, CIFAR-10 Batch 3:  Accuracy:  0.656 Cost:  0.784851\n",
      "Epoch 48, CIFAR-10 Batch 4:  Accuracy:  0.6812 Cost:  0.751687\n",
      "Epoch 48, CIFAR-10 Batch 5:  Accuracy:  0.6592 Cost:  0.820006\n",
      "Epoch 49, CIFAR-10 Batch 1:  Accuracy:  0.6672 Cost:  0.887785\n",
      "Epoch 49, CIFAR-10 Batch 2:  Accuracy:  0.6414 Cost:  0.913647\n",
      "Epoch 49, CIFAR-10 Batch 3:  Accuracy:  0.6526 Cost:  0.783253\n",
      "Epoch 49, CIFAR-10 Batch 4:  Accuracy:  0.6566 Cost:  0.794728\n",
      "Epoch 49, CIFAR-10 Batch 5:  Accuracy:  0.6462 Cost:  0.846509\n",
      "Epoch 50, CIFAR-10 Batch 1:  Accuracy:  0.6794 Cost:  0.859604\n",
      "Epoch 50, CIFAR-10 Batch 2:  Accuracy:  0.6752 Cost:  0.818487\n",
      "Epoch 50, CIFAR-10 Batch 3:  Accuracy:  0.6682 Cost:  0.76075\n",
      "Epoch 50, CIFAR-10 Batch 4:  Accuracy:  0.6788 Cost:  0.740784\n",
      "Epoch 50, CIFAR-10 Batch 5:  Accuracy:  0.6676 Cost:  0.787779\n",
      "Epoch 51, CIFAR-10 Batch 1:  Accuracy:  0.6852 Cost:  0.857375\n",
      "Epoch 51, CIFAR-10 Batch 2:  Accuracy:  0.679 Cost:  0.821841\n",
      "Epoch 51, CIFAR-10 Batch 3:  Accuracy:  0.6434 Cost:  0.801147\n",
      "Epoch 51, CIFAR-10 Batch 4:  Accuracy:  0.6818 Cost:  0.712272\n",
      "Epoch 51, CIFAR-10 Batch 5:  Accuracy:  0.668 Cost:  0.785803\n",
      "Epoch 52, CIFAR-10 Batch 1:  Accuracy:  0.6676 Cost:  0.902718\n",
      "Epoch 52, CIFAR-10 Batch 2:  Accuracy:  0.6824 Cost:  0.808126\n",
      "Epoch 52, CIFAR-10 Batch 3:  Accuracy:  0.6826 Cost:  0.710575\n",
      "Epoch 52, CIFAR-10 Batch 4:  Accuracy:  0.67 Cost:  0.750484\n",
      "Epoch 52, CIFAR-10 Batch 5:  Accuracy:  0.6786 Cost:  0.789517\n",
      "Epoch 53, CIFAR-10 Batch 1:  Accuracy:  0.6932 Cost:  0.800171\n",
      "Epoch 53, CIFAR-10 Batch 2:  Accuracy:  0.688 Cost:  0.780086\n",
      "Epoch 53, CIFAR-10 Batch 3:  Accuracy:  0.661 Cost:  0.751144\n",
      "Epoch 53, CIFAR-10 Batch 4:  Accuracy:  0.671 Cost:  0.750303\n",
      "Epoch 53, CIFAR-10 Batch 5:  Accuracy:  0.6812 Cost:  0.741851\n",
      "Epoch 54, CIFAR-10 Batch 1:  Accuracy:  0.6808 Cost:  0.82069\n",
      "Epoch 54, CIFAR-10 Batch 2:  Accuracy:  0.6858 Cost:  0.791073\n",
      "Epoch 54, CIFAR-10 Batch 3:  Accuracy:  0.6798 Cost:  0.722799\n",
      "Epoch 54, CIFAR-10 Batch 4:  Accuracy:  0.6732 Cost:  0.745866\n",
      "Epoch 54, CIFAR-10 Batch 5:  Accuracy:  0.6932 Cost:  0.729365\n",
      "Epoch 55, CIFAR-10 Batch 1:  Accuracy:  0.6886 Cost:  0.847444\n",
      "Epoch 55, CIFAR-10 Batch 2:  Accuracy:  0.6934 Cost:  0.769387\n",
      "Epoch 55, CIFAR-10 Batch 3:  Accuracy:  0.68 Cost:  0.720306\n",
      "Epoch 55, CIFAR-10 Batch 4:  Accuracy:  0.6828 Cost:  0.714909\n",
      "Epoch 55, CIFAR-10 Batch 5:  Accuracy:  0.6884 Cost:  0.734911\n",
      "Epoch 56, CIFAR-10 Batch 1:  Accuracy:  0.6908 Cost:  0.814094\n",
      "Epoch 56, CIFAR-10 Batch 2:  Accuracy:  0.6918 Cost:  0.768105\n",
      "Epoch 56, CIFAR-10 Batch 3:  Accuracy:  0.6658 Cost:  0.734118\n",
      "Epoch 56, CIFAR-10 Batch 4:  Accuracy:  0.683 Cost:  0.69418\n",
      "Epoch 56, CIFAR-10 Batch 5:  Accuracy:  0.6676 Cost:  0.777308\n",
      "Epoch 57, CIFAR-10 Batch 1:  Accuracy:  0.6966 Cost:  0.801283\n",
      "Epoch 57, CIFAR-10 Batch 2:  Accuracy:  0.6932 Cost:  0.757665\n",
      "Epoch 57, CIFAR-10 Batch 3:  Accuracy:  0.6826 Cost:  0.698718\n",
      "Epoch 57, CIFAR-10 Batch 4:  Accuracy:  0.6834 Cost:  0.732728\n",
      "Epoch 57, CIFAR-10 Batch 5:  Accuracy:  0.6922 Cost:  0.739664\n",
      "Epoch 58, CIFAR-10 Batch 1:  Accuracy:  0.6854 Cost:  0.808935\n",
      "Epoch 58, CIFAR-10 Batch 2:  Accuracy:  0.6966 Cost:  0.732356\n",
      "Epoch 58, CIFAR-10 Batch 3:  Accuracy:  0.6834 Cost:  0.689769\n",
      "Epoch 58, CIFAR-10 Batch 4:  Accuracy:  0.6912 Cost:  0.697278\n",
      "Epoch 58, CIFAR-10 Batch 5:  Accuracy:  0.6912 Cost:  0.718516\n",
      "Epoch 59, CIFAR-10 Batch 1:  Accuracy:  0.6904 Cost:  0.786051\n",
      "Epoch 59, CIFAR-10 Batch 2:  Accuracy:  0.6934 Cost:  0.756682\n",
      "Epoch 59, CIFAR-10 Batch 3:  Accuracy:  0.6748 Cost:  0.710893\n",
      "Epoch 59, CIFAR-10 Batch 4:  Accuracy:  0.6982 Cost:  0.677456\n",
      "Epoch 59, CIFAR-10 Batch 5:  Accuracy:  0.6854 Cost:  0.716346\n",
      "Epoch 60, CIFAR-10 Batch 1:  Accuracy:  0.6984 Cost:  0.753522\n",
      "Epoch 60, CIFAR-10 Batch 2:  Accuracy:  0.6868 Cost:  0.767705\n",
      "Epoch 60, CIFAR-10 Batch 3:  Accuracy:  0.6676 Cost:  0.732614\n",
      "Epoch 60, CIFAR-10 Batch 4:  Accuracy:  0.7018 Cost:  0.652054\n",
      "Epoch 60, CIFAR-10 Batch 5:  Accuracy:  0.7146 Cost:  0.664695\n",
      "Epoch 61, CIFAR-10 Batch 1:  Accuracy:  0.7068 Cost:  0.750674\n",
      "Epoch 61, CIFAR-10 Batch 2:  Accuracy:  0.6892 Cost:  0.746656\n",
      "Epoch 61, CIFAR-10 Batch 3:  Accuracy:  0.6894 Cost:  0.687133\n",
      "Epoch 61, CIFAR-10 Batch 4:  Accuracy:  0.6902 Cost:  0.671896\n",
      "Epoch 61, CIFAR-10 Batch 5:  Accuracy:  0.7028 Cost:  0.680446\n",
      "Epoch 62, CIFAR-10 Batch 1:  Accuracy:  0.705 Cost:  0.745199\n",
      "Epoch 62, CIFAR-10 Batch 2:  Accuracy:  0.6942 Cost:  0.730087\n",
      "Epoch 62, CIFAR-10 Batch 3:  Accuracy:  0.6866 Cost:  0.672938\n",
      "Epoch 62, CIFAR-10 Batch 4:  Accuracy:  0.6956 Cost:  0.669724\n",
      "Epoch 62, CIFAR-10 Batch 5:  Accuracy:  0.694 Cost:  0.697171\n",
      "Epoch 63, CIFAR-10 Batch 1:  Accuracy:  0.7028 Cost:  0.736803\n",
      "Epoch 63, CIFAR-10 Batch 2:  Accuracy:  0.699 Cost:  0.720884\n",
      "Epoch 63, CIFAR-10 Batch 3:  Accuracy:  0.6928 Cost:  0.674412\n",
      "Epoch 63, CIFAR-10 Batch 4:  Accuracy:  0.703 Cost:  0.6493\n",
      "Epoch 63, CIFAR-10 Batch 5:  Accuracy:  0.6968 Cost:  0.667111\n",
      "Epoch 64, CIFAR-10 Batch 1:  Accuracy:  0.7112 Cost:  0.753559\n",
      "Epoch 64, CIFAR-10 Batch 2:  Accuracy:  0.7056 Cost:  0.696689\n",
      "Epoch 64, CIFAR-10 Batch 3:  Accuracy:  0.6996 Cost:  0.648632\n",
      "Epoch 64, CIFAR-10 Batch 4:  Accuracy:  0.7008 Cost:  0.648306\n",
      "Epoch 64, CIFAR-10 Batch 5:  Accuracy:  0.7024 Cost:  0.661448\n",
      "Epoch 65, CIFAR-10 Batch 1:  Accuracy:  0.705 Cost:  0.762509\n",
      "Epoch 65, CIFAR-10 Batch 2:  Accuracy:  0.6898 Cost:  0.717359\n",
      "Epoch 65, CIFAR-10 Batch 3:  Accuracy:  0.699 Cost:  0.630873\n",
      "Epoch 65, CIFAR-10 Batch 4:  Accuracy:  0.7158 Cost:  0.63491\n",
      "Epoch 65, CIFAR-10 Batch 5:  Accuracy:  0.706 Cost:  0.642803\n",
      "Epoch 66, CIFAR-10 Batch 1:  Accuracy:  0.6986 Cost:  0.750367\n",
      "Epoch 66, CIFAR-10 Batch 2:  Accuracy:  0.701 Cost:  0.699587\n",
      "Epoch 66, CIFAR-10 Batch 3:  Accuracy:  0.7002 Cost:  0.633271\n",
      "Epoch 66, CIFAR-10 Batch 4:  Accuracy:  0.6986 Cost:  0.655808\n",
      "Epoch 66, CIFAR-10 Batch 5:  Accuracy:  0.7028 Cost:  0.628633\n",
      "Epoch 67, CIFAR-10 Batch 1:  Accuracy:  0.7138 Cost:  0.701407\n",
      "Epoch 67, CIFAR-10 Batch 2:  Accuracy:  0.6818 Cost:  0.746891\n",
      "Epoch 67, CIFAR-10 Batch 3:  Accuracy:  0.6944 Cost:  0.653136\n",
      "Epoch 67, CIFAR-10 Batch 4:  Accuracy:  0.7052 Cost:  0.638418\n",
      "Epoch 67, CIFAR-10 Batch 5:  Accuracy:  0.689 Cost:  0.657244\n",
      "Epoch 68, CIFAR-10 Batch 1:  Accuracy:  0.704 Cost:  0.733807\n",
      "Epoch 68, CIFAR-10 Batch 2:  Accuracy:  0.715 Cost:  0.673244\n",
      "Epoch 68, CIFAR-10 Batch 3:  Accuracy:  0.6956 Cost:  0.634937\n",
      "Epoch 68, CIFAR-10 Batch 4:  Accuracy:  0.7148 Cost:  0.603323\n",
      "Epoch 68, CIFAR-10 Batch 5:  Accuracy:  0.7024 Cost:  0.623909\n",
      "Epoch 69, CIFAR-10 Batch 1:  Accuracy:  0.712 Cost:  0.720746\n",
      "Epoch 69, CIFAR-10 Batch 2:  Accuracy:  0.7182 Cost:  0.663425\n",
      "Epoch 69, CIFAR-10 Batch 3:  Accuracy:  0.7044 Cost:  0.637095\n",
      "Epoch 69, CIFAR-10 Batch 4:  Accuracy:  0.7036 Cost:  0.609242\n",
      "Epoch 69, CIFAR-10 Batch 5:  Accuracy:  0.7212 Cost:  0.603631\n",
      "Epoch 70, CIFAR-10 Batch 1:  Accuracy:  0.7212 Cost:  0.696039\n",
      "Epoch 70, CIFAR-10 Batch 2:  Accuracy:  0.7146 Cost:  0.658443\n",
      "Epoch 70, CIFAR-10 Batch 3:  Accuracy:  0.6994 Cost:  0.645243\n",
      "Epoch 70, CIFAR-10 Batch 4:  Accuracy:  0.7082 Cost:  0.609182\n",
      "Epoch 70, CIFAR-10 Batch 5:  Accuracy:  0.7058 Cost:  0.643348\n",
      "Epoch 71, CIFAR-10 Batch 1:  Accuracy:  0.7098 Cost:  0.714525\n",
      "Epoch 71, CIFAR-10 Batch 2:  Accuracy:  0.7094 Cost:  0.673759\n",
      "Epoch 71, CIFAR-10 Batch 3:  Accuracy:  0.7182 Cost:  0.609582\n",
      "Epoch 71, CIFAR-10 Batch 4:  Accuracy:  0.6992 Cost:  0.633632\n",
      "Epoch 71, CIFAR-10 Batch 5:  Accuracy:  0.7016 Cost:  0.640846\n",
      "Epoch 72, CIFAR-10 Batch 1:  Accuracy:  0.721 Cost:  0.651245\n",
      "Epoch 72, CIFAR-10 Batch 2:  Accuracy:  0.7198 Cost:  0.641916\n",
      "Epoch 72, CIFAR-10 Batch 3:  Accuracy:  0.7106 Cost:  0.596503\n",
      "Epoch 72, CIFAR-10 Batch 4:  Accuracy:  0.7168 Cost:  0.586374\n",
      "Epoch 72, CIFAR-10 Batch 5:  Accuracy:  0.6918 Cost:  0.655373\n",
      "Epoch 73, CIFAR-10 Batch 1:  Accuracy:  0.7214 Cost:  0.652325\n",
      "Epoch 73, CIFAR-10 Batch 2:  Accuracy:  0.7136 Cost:  0.664744\n",
      "Epoch 73, CIFAR-10 Batch 3:  Accuracy:  0.7166 Cost:  0.60201\n",
      "Epoch 73, CIFAR-10 Batch 4:  Accuracy:  0.7128 Cost:  0.589578\n",
      "Epoch 73, CIFAR-10 Batch 5:  Accuracy:  0.7188 Cost:  0.598236\n",
      "Epoch 74, CIFAR-10 Batch 1:  Accuracy:  0.7226 Cost:  0.685494\n",
      "Epoch 74, CIFAR-10 Batch 2:  Accuracy:  0.7204 Cost:  0.650247\n",
      "Epoch 74, CIFAR-10 Batch 3:  Accuracy:  0.7106 Cost:  0.606741\n",
      "Epoch 74, CIFAR-10 Batch 4:  Accuracy:  0.7102 Cost:  0.597283\n",
      "Epoch 74, CIFAR-10 Batch 5:  Accuracy:  0.7064 Cost:  0.603367\n",
      "Epoch 75, CIFAR-10 Batch 1:  Accuracy:  0.7202 Cost:  0.673878\n",
      "Epoch 75, CIFAR-10 Batch 2:  Accuracy:  0.7168 Cost:  0.645024\n",
      "Epoch 75, CIFAR-10 Batch 3:  Accuracy:  0.6968 Cost:  0.637541\n",
      "Epoch 75, CIFAR-10 Batch 4:  Accuracy:  0.7176 Cost:  0.557429\n",
      "Epoch 75, CIFAR-10 Batch 5:  Accuracy:  0.7126 Cost:  0.598942\n",
      "Epoch 76, CIFAR-10 Batch 1:  Accuracy:  0.7184 Cost:  0.708541\n",
      "Epoch 76, CIFAR-10 Batch 2:  Accuracy:  0.723 Cost:  0.645346\n",
      "Epoch 76, CIFAR-10 Batch 3:  Accuracy:  0.714 Cost:  0.588321\n",
      "Epoch 76, CIFAR-10 Batch 4:  Accuracy:  0.7246 Cost:  0.57267\n",
      "Epoch 76, CIFAR-10 Batch 5:  Accuracy:  0.7154 Cost:  0.582869\n",
      "Epoch 77, CIFAR-10 Batch 1:  Accuracy:  0.7182 Cost:  0.669514\n",
      "Epoch 77, CIFAR-10 Batch 2:  Accuracy:  0.7218 Cost:  0.637226\n",
      "Epoch 77, CIFAR-10 Batch 3:  Accuracy:  0.717 Cost:  0.586083\n",
      "Epoch 77, CIFAR-10 Batch 4:  Accuracy:  0.7138 Cost:  0.563516\n",
      "Epoch 77, CIFAR-10 Batch 5:  Accuracy:  0.7136 Cost:  0.594849\n",
      "Epoch 78, CIFAR-10 Batch 1:  Accuracy:  0.7128 Cost:  0.685668\n",
      "Epoch 78, CIFAR-10 Batch 2:  Accuracy:  0.7238 Cost:  0.636402\n",
      "Epoch 78, CIFAR-10 Batch 3:  Accuracy:  0.7112 Cost:  0.609652\n",
      "Epoch 78, CIFAR-10 Batch 4:  Accuracy:  0.72 Cost:  0.557717\n",
      "Epoch 78, CIFAR-10 Batch 5:  Accuracy:  0.7194 Cost:  0.567876\n",
      "Epoch 79, CIFAR-10 Batch 1:  Accuracy:  0.721 Cost:  0.668582\n",
      "Epoch 79, CIFAR-10 Batch 2:  Accuracy:  0.7266 Cost:  0.602163\n",
      "Epoch 79, CIFAR-10 Batch 3:  Accuracy:  0.7054 Cost:  0.619654\n",
      "Epoch 79, CIFAR-10 Batch 4:  Accuracy:  0.7284 Cost:  0.554158\n",
      "Epoch 79, CIFAR-10 Batch 5:  Accuracy:  0.7202 Cost:  0.559786\n",
      "Epoch 80, CIFAR-10 Batch 1:  Accuracy:  0.719 Cost:  0.671182\n",
      "Epoch 80, CIFAR-10 Batch 2:  Accuracy:  0.7182 Cost:  0.643544\n",
      "Epoch 80, CIFAR-10 Batch 3:  Accuracy:  0.7204 Cost:  0.584114\n",
      "Epoch 80, CIFAR-10 Batch 4:  Accuracy:  0.7262 Cost:  0.551827\n",
      "Epoch 80, CIFAR-10 Batch 5:  Accuracy:  0.723 Cost:  0.560164\n",
      "Epoch 81, CIFAR-10 Batch 1:  Accuracy:  0.7328 Cost:  0.627365\n",
      "Epoch 81, CIFAR-10 Batch 2:  Accuracy:  0.7336 Cost:  0.597172\n",
      "Epoch 81, CIFAR-10 Batch 3:  Accuracy:  0.7242 Cost:  0.566203\n",
      "Epoch 81, CIFAR-10 Batch 4:  Accuracy:  0.7218 Cost:  0.571503\n",
      "Epoch 81, CIFAR-10 Batch 5:  Accuracy:  0.713 Cost:  0.590425\n",
      "Epoch 82, CIFAR-10 Batch 1:  Accuracy:  0.7306 Cost:  0.632808\n",
      "Epoch 82, CIFAR-10 Batch 2:  Accuracy:  0.728 Cost:  0.608812\n",
      "Epoch 82, CIFAR-10 Batch 3:  Accuracy:  0.72 Cost:  0.564993\n",
      "Epoch 82, CIFAR-10 Batch 4:  Accuracy:  0.7346 Cost:  0.548729\n",
      "Epoch 82, CIFAR-10 Batch 5:  Accuracy:  0.7206 Cost:  0.564393\n",
      "Epoch 83, CIFAR-10 Batch 1:  Accuracy:  0.739 Cost:  0.6171\n",
      "Epoch 83, CIFAR-10 Batch 2:  Accuracy:  0.7334 Cost:  0.586251\n",
      "Epoch 83, CIFAR-10 Batch 3:  Accuracy:  0.7096 Cost:  0.596821\n",
      "Epoch 83, CIFAR-10 Batch 4:  Accuracy:  0.7352 Cost:  0.530646\n",
      "Epoch 83, CIFAR-10 Batch 5:  Accuracy:  0.701 Cost:  0.602186\n",
      "Epoch 84, CIFAR-10 Batch 1:  Accuracy:  0.728 Cost:  0.609583\n",
      "Epoch 84, CIFAR-10 Batch 2:  Accuracy:  0.731 Cost:  0.589687\n",
      "Epoch 84, CIFAR-10 Batch 3:  Accuracy:  0.7186 Cost:  0.571937\n",
      "Epoch 84, CIFAR-10 Batch 4:  Accuracy:  0.7348 Cost:  0.531313\n",
      "Epoch 84, CIFAR-10 Batch 5:  Accuracy:  0.7234 Cost:  0.548427\n",
      "Epoch 85, CIFAR-10 Batch 1:  Accuracy:  0.7236 Cost:  0.609339\n",
      "Epoch 85, CIFAR-10 Batch 2:  Accuracy:  0.723 Cost:  0.610417\n",
      "Epoch 85, CIFAR-10 Batch 3:  Accuracy:  0.72 Cost:  0.563013\n",
      "Epoch 85, CIFAR-10 Batch 4:  Accuracy:  0.7312 Cost:  0.520674\n",
      "Epoch 85, CIFAR-10 Batch 5:  Accuracy:  0.7326 Cost:  0.521035\n",
      "Epoch 86, CIFAR-10 Batch 1:  Accuracy:  0.7362 Cost:  0.602059\n",
      "Epoch 86, CIFAR-10 Batch 2:  Accuracy:  0.723 Cost:  0.581836\n",
      "Epoch 86, CIFAR-10 Batch 3:  Accuracy:  0.7328 Cost:  0.512661\n",
      "Epoch 86, CIFAR-10 Batch 4:  Accuracy:  0.735 Cost:  0.506125\n",
      "Epoch 86, CIFAR-10 Batch 5:  Accuracy:  0.729 Cost:  0.53029\n",
      "Epoch 87, CIFAR-10 Batch 1:  Accuracy:  0.744 Cost:  0.580269\n",
      "Epoch 87, CIFAR-10 Batch 2:  Accuracy:  0.7306 Cost:  0.578613\n",
      "Epoch 87, CIFAR-10 Batch 3:  Accuracy:  0.733 Cost:  0.551978\n",
      "Epoch 87, CIFAR-10 Batch 4:  Accuracy:  0.739 Cost:  0.520928\n",
      "Epoch 87, CIFAR-10 Batch 5:  Accuracy:  0.724 Cost:  0.551203\n",
      "Epoch 88, CIFAR-10 Batch 1:  Accuracy:  0.737 Cost:  0.598346\n",
      "Epoch 88, CIFAR-10 Batch 2:  Accuracy:  0.7288 Cost:  0.581781\n",
      "Epoch 88, CIFAR-10 Batch 3:  Accuracy:  0.727 Cost:  0.571912\n",
      "Epoch 88, CIFAR-10 Batch 4:  Accuracy:  0.7472 Cost:  0.501857\n",
      "Epoch 88, CIFAR-10 Batch 5:  Accuracy:  0.7274 Cost:  0.515567\n",
      "Epoch 89, CIFAR-10 Batch 1:  Accuracy:  0.7296 Cost:  0.603739\n",
      "Epoch 89, CIFAR-10 Batch 2:  Accuracy:  0.7404 Cost:  0.543737\n",
      "Epoch 89, CIFAR-10 Batch 3:  Accuracy:  0.7198 Cost:  0.531113\n",
      "Epoch 89, CIFAR-10 Batch 4:  Accuracy:  0.733 Cost:  0.509404\n",
      "Epoch 89, CIFAR-10 Batch 5:  Accuracy:  0.7384 Cost:  0.500826\n",
      "Epoch 90, CIFAR-10 Batch 1:  Accuracy:  0.745 Cost:  0.578239\n",
      "Epoch 90, CIFAR-10 Batch 2:  Accuracy:  0.7176 Cost:  0.589472\n",
      "Epoch 90, CIFAR-10 Batch 3:  Accuracy:  0.7304 Cost:  0.525694\n",
      "Epoch 90, CIFAR-10 Batch 4:  Accuracy:  0.7248 Cost:  0.52365\n",
      "Epoch 90, CIFAR-10 Batch 5:  Accuracy:  0.7188 Cost:  0.539912\n",
      "Epoch 91, CIFAR-10 Batch 1:  Accuracy:  0.7372 Cost:  0.620519\n",
      "Epoch 91, CIFAR-10 Batch 2:  Accuracy:  0.7374 Cost:  0.551623\n",
      "Epoch 91, CIFAR-10 Batch 3:  Accuracy:  0.7158 Cost:  0.553232\n",
      "Epoch 91, CIFAR-10 Batch 4:  Accuracy:  0.7234 Cost:  0.510884\n",
      "Epoch 91, CIFAR-10 Batch 5:  Accuracy:  0.7298 Cost:  0.520155\n",
      "Epoch 92, CIFAR-10 Batch 1:  Accuracy:  0.7264 Cost:  0.59261\n",
      "Epoch 92, CIFAR-10 Batch 2:  Accuracy:  0.7372 Cost:  0.537174\n",
      "Epoch 92, CIFAR-10 Batch 3:  Accuracy:  0.73 Cost:  0.523936\n",
      "Epoch 92, CIFAR-10 Batch 4:  Accuracy:  0.7318 Cost:  0.507213\n",
      "Epoch 92, CIFAR-10 Batch 5:  Accuracy:  0.74 Cost:  0.493127\n",
      "Epoch 93, CIFAR-10 Batch 1:  Accuracy:  0.7378 Cost:  0.607289\n",
      "Epoch 93, CIFAR-10 Batch 2:  Accuracy:  0.7422 Cost:  0.534798\n",
      "Epoch 93, CIFAR-10 Batch 3:  Accuracy:  0.7288 Cost:  0.511668\n",
      "Epoch 93, CIFAR-10 Batch 4:  Accuracy:  0.7226 Cost:  0.533929\n",
      "Epoch 93, CIFAR-10 Batch 5:  Accuracy:  0.7438 Cost:  0.488692\n",
      "Epoch 94, CIFAR-10 Batch 1:  Accuracy:  0.7348 Cost:  0.595155\n",
      "Epoch 94, CIFAR-10 Batch 2:  Accuracy:  0.7384 Cost:  0.538625\n",
      "Epoch 94, CIFAR-10 Batch 3:  Accuracy:  0.7218 Cost:  0.522192\n",
      "Epoch 94, CIFAR-10 Batch 4:  Accuracy:  0.725 Cost:  0.509009\n",
      "Epoch 94, CIFAR-10 Batch 5:  Accuracy:  0.7318 Cost:  0.512751\n",
      "Epoch 95, CIFAR-10 Batch 1:  Accuracy:  0.745 Cost:  0.581087\n",
      "Epoch 95, CIFAR-10 Batch 2:  Accuracy:  0.7432 Cost:  0.520265\n",
      "Epoch 95, CIFAR-10 Batch 3:  Accuracy:  0.7262 Cost:  0.52542\n",
      "Epoch 95, CIFAR-10 Batch 4:  Accuracy:  0.7436 Cost:  0.472828\n",
      "Epoch 95, CIFAR-10 Batch 5:  Accuracy:  0.7404 Cost:  0.484456\n",
      "Epoch 96, CIFAR-10 Batch 1:  Accuracy:  0.7428 Cost:  0.560919\n",
      "Epoch 96, CIFAR-10 Batch 2:  Accuracy:  0.7288 Cost:  0.549801\n",
      "Epoch 96, CIFAR-10 Batch 3:  Accuracy:  0.7164 Cost:  0.567027\n",
      "Epoch 96, CIFAR-10 Batch 4:  Accuracy:  0.7446 Cost:  0.463832\n",
      "Epoch 96, CIFAR-10 Batch 5:  Accuracy:  0.7452 Cost:  0.477571\n",
      "Epoch 97, CIFAR-10 Batch 1:  Accuracy:  0.741 Cost:  0.586969\n",
      "Epoch 97, CIFAR-10 Batch 2:  Accuracy:  0.7082 Cost:  0.586175\n",
      "Epoch 97, CIFAR-10 Batch 3:  Accuracy:  0.737 Cost:  0.510875\n",
      "Epoch 97, CIFAR-10 Batch 4:  Accuracy:  0.7348 Cost:  0.496774\n",
      "Epoch 97, CIFAR-10 Batch 5:  Accuracy:  0.7304 Cost:  0.514101\n",
      "Epoch 98, CIFAR-10 Batch 1:  Accuracy:  0.7416 Cost:  0.581454\n",
      "Epoch 98, CIFAR-10 Batch 2:  Accuracy:  0.7406 Cost:  0.513828\n",
      "Epoch 98, CIFAR-10 Batch 3:  Accuracy:  0.7334 Cost:  0.514917\n",
      "Epoch 98, CIFAR-10 Batch 4:  Accuracy:  0.7208 Cost:  0.506852\n",
      "Epoch 98, CIFAR-10 Batch 5:  Accuracy:  0.7374 Cost:  0.477772\n",
      "Epoch 99, CIFAR-10 Batch 1:  Accuracy:  0.7512 Cost:  0.55101\n",
      "Epoch 99, CIFAR-10 Batch 2:  Accuracy:  0.7338 Cost:  0.545474\n",
      "Epoch 99, CIFAR-10 Batch 3:  Accuracy:  0.7302 Cost:  0.506117\n",
      "Epoch 99, CIFAR-10 Batch 4:  Accuracy:  0.751 Cost:  0.450153\n",
      "Epoch 99, CIFAR-10 Batch 5:  Accuracy:  0.7536 Cost:  0.443928\n",
      "Epoch 100, CIFAR-10 Batch 1:  Accuracy:  0.7448 Cost:  0.552674\n",
      "Epoch 100, CIFAR-10 Batch 2:  Accuracy:  0.7294 Cost:  0.524423\n",
      "Epoch 100, CIFAR-10 Batch 3:  Accuracy:  0.74 Cost:  0.508283\n",
      "Epoch 100, CIFAR-10 Batch 4:  Accuracy:  0.732 Cost:  0.481671\n",
      "Epoch 100, CIFAR-10 Batch 5:  Accuracy:  0.73 Cost:  0.501199\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
